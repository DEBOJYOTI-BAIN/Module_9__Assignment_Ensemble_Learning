{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "  - Ensemble Learning is a powerful machine learning technique in which multiple individual models (called base learners or weak learners) are combined to produce a single, stronger predictive model. Instead of relying on one model, ensemble learning leverages the collective wisdom of several models to improve accuracy, stability, and generalization.\n",
        "\n",
        "  - The key idea behind ensemble learning is that a group of weak models, when combined properly, can perform better than any single model alone. Each individual model may make errors in different areas, but when their predictions are aggregated through methods such as voting, averaging, or weighted combinations, the overall error tends to reduce. This concept is similar to the “wisdom of the crowd,” where diverse opinions lead to better decisions.\n",
        "\n",
        "  - There are three main types of ensemble methods:\n",
        "\n",
        "    - Bagging (Bootstrap Aggregating): Reduces variance by training multiple models on random subsets of the data and averaging their predictions. Example: Random Forest.\n",
        "\n",
        "    - Boosting: Reduces bias by training models sequentially, where each model focuses on correcting the errors of the previous ones. Example: AdaBoost, XGBoost.\n",
        "\n",
        "    - Stacking: Combines predictions from multiple models using another meta-model that learns the optimal way to blend them.\n",
        "\n",
        "  - Advantages:\n",
        "\n",
        "    - Improves predictive accuracy and robustness.\n",
        "\n",
        "    - Reduces overfitting (especially in bagging methods).\n",
        "\n",
        "    - Works well for complex real-world datasets.\n",
        "\n",
        "  - Example:\n",
        "    - A Random Forest combines many Decision Trees, each trained on different data samples and features. By averaging their results, it delivers more stable and accurate predictions than any single tree.\n",
        "\n",
        "2.  What is the difference between Bagging and Boosting?\n",
        "  - Bagging (Bootstrap Aggregating) trains multiple models independently on different random subsets of data and combines their results by averaging (for regression) or majority voting (for classification). It mainly helps to reduce variance and prevent overfitting.\n",
        "\n",
        "  - Boosting, on the other hand, trains models sequentially, where each new model focuses on correcting the errors of the previous ones. It helps to reduce bias and improve accuracy but can overfit if not tuned properly.\n",
        "\n",
        "  - Example:\n",
        "\n",
        "    - Bagging → Random Forest\n",
        "\n",
        "    - Boosting → AdaBoost, XGBoost\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "  - Bootstrap sampling is a statistical technique where random samples are drawn from the original dataset with replacement. This means some data points may appear multiple times in one sample, while others may not appear at all.\n",
        "  - In Bagging methods like Random Forest, bootstrap sampling is used to create different training subsets for each model (or decision tree).\n",
        "  - Each tree is trained on its own unique bootstrap sample, which introduces diversity among the models.\n",
        "  - This randomness helps reduce variance and prevents overfitting, making the final ensemble (like Random Forest) more stable and accurate.\n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "  - Out-of-Bag (OOB) samples are the data points not included in a particular bootstrap sample during the training of a model in ensemble methods like Random Forest. Since each tree is trained on about two-thirds of the data (due to sampling with replacement), the remaining one-third becomes its OOB data.\n",
        "\n",
        "  - The OOB score is an internal validation method that measures model accuracy using these OOB samples — each data point is tested only on trees that did not see it during training.\n",
        "\n",
        "  - This provides an unbiased estimate of model performance without needing a separate validation set, saving data and computation.\n",
        "\n",
        "5.  Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "  - In a single Decision Tree, feature importance is calculated based on how much each feature reduces impurity (like Gini impurity or entropy) across all its splits. The higher the reduction, the more important the feature is. However, since the tree is built on one dataset, its feature importance can be unstable and may vary with small data changes.\n",
        "\n",
        "  - In a Random Forest, feature importance is computed by averaging the importance scores of each feature across all the trees in the forest. This aggregation makes the importance scores more reliable, robust, and less sensitive to noise.\n",
        "\n",
        "  - Thus, Random Forest gives a more general and stable measure of which features are truly important for prediction."
      ],
      "metadata": {
        "id": "miO8HjiTqXFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to:\n",
        "#● Load the Breast Cancer dataset using\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "features = data.feature_names\n",
        "\n",
        "# Create a DataFrame and display top 5 features\n",
        "feat_importance = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "top5 = feat_importance.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghH6s3zesDYj",
        "outputId": "fc9e737b-a6a5-48e7-fee3-4fb000985535"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.153892\n",
            "27  worst concave points    0.144663\n",
            "7    mean concave points    0.106210\n",
            "20          worst radius    0.077987\n",
            "6         mean concavity    0.068001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Train a Bagging Classifier (updated syntax)\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z--QfApIsPy-",
        "outputId": "80f55cd1-9c33-42a9-8c83-edca24d5eea4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Perform Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38YKuclTsd-g",
        "outputId": "9e4eacc7-8036-472e-e3f4-61ed4c7f501e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 150}\n",
            "Final Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor using Decision Trees as base estimators\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_pred = bag_reg.predict(X_test)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Errors\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Print comparison results\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K89rMtDKsv4t",
        "outputId": "f144e778-24af-4f44-989a-d7a1aa7a1eb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "  - Choosing between Bagging & Boosting:\n",
        "\n",
        "    - Use Bagging (Random Forest) to reduce variance and handle noisy data.\n",
        "\n",
        "    - Use Boosting (XGBoost, AdaBoost) when accuracy and bias reduction are key.\n",
        "\n",
        "  - Handling Overfitting:\n",
        "\n",
        "    - Apply cross-validation, limit tree depth, use regularization, and tune hyperparameters.\n",
        "\n",
        "    - Early stopping (in boosting) and pruning help prevent overfitting.\n",
        "\n",
        "  - Selecting Base Models:\n",
        "\n",
        "    - Start with Decision Trees as base learners.\n",
        "\n",
        "    - For stacking, combine diverse models like Logistic Regression, Random Forest, and XGBoost.\n",
        "\n",
        "  - Evaluating Performance:\n",
        "\n",
        "    - Use k-fold cross-validation to ensure model stability.\n",
        "\n",
        "    - Measure metrics like Accuracy, Precision, Recall, F1-Score, and AUC.\n",
        "\n",
        "  - Justification (Real-world impact):\n",
        "\n",
        "    - Ensemble learning improves prediction accuracy, reduces risk of misclassification, and helps the bank make better loan approval decisions by combining multiple model strengths."
      ],
      "metadata": {
        "id": "rAHJZ1S-tSzK"
      }
    }
  ]
}